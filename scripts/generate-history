#!/usr/bin/env python3
from collections import OrderedDict
from datetime import datetime
from fnmatch import fnmatch
import argparse
import glob
import hashlib
import json
import logging
import os
import sys

logger = logging.getLogger(__name__)
log_handler = logging.StreamHandler(sys.stdout)
log_handler.setLevel(logging.INFO)
logger.addHandler(log_handler)
logger.setLevel(logging.INFO)

arg_parser = argparse.ArgumentParser(description='Generates a JSON history of modified files.')
arg_parser.add_argument(
    'source_dir',
    help='The source directory to read', 
    type=str,
)
arg_parser.add_argument(
    'output_path',
    help='The output file where JSON will be written', 
    type=str,
)
args = arg_parser.parse_args()

extensions_schemas = {
    'aac': 'file.audio',
    'avi': 'file.video',
    'css': 'file.code.css',
    'gif': 'file.image.gif',
    'html': 'file.document.html',
    'jpeg': 'file.image.jpg',
    'jpg': 'file.image.jpg',
    'js': 'file.code.javascript',
    'm4a': 'file.audio',
    'm4v': 'file.video',
    'md': 'file.document.markdown',
    'mkv': 'file.video',
    'mov': 'file.video',
    'mp3': 'file.audio',
    'mp4': 'file.video',
    'mpeg': 'file.video',
    'mpg': 'file.video',
    'pdf': 'file.document.pdf',
    'php': 'file.code.php',
    'png': 'file.image.png',
    'py': 'file.code.python',
    'sh': 'file.code.bash',
    'tiff': 'file.image.tiff',
    'txt': 'file.document.text',
    'wav': 'file.audio',
    'webm': 'file.video',
    'webp': 'file.image.webp',
    'wmv': 'file.video',
}


def add_entry(entries_by_date, file_entry):
    key = file_entry['dateUpdated'][0:10]
    if key not in entries_by_date:
        entries_by_date[key] = []
    entries_by_date[key].append(file_entry)


def get_normalized_extension(path):
    return os.path.splitext(path)[1][1:].lower()


def get_schema(path):
    return extensions_schemas.get(get_normalized_extension(path), 'file')


def is_included(path):
    return get_normalized_extension(path) in extensions_schemas


def to_json_datetime(datetime):
    return datetime.isoformat()[:19]


def generate_checksum(path):
    with open(path, "rb") as f:
        file_hash = hashlib.blake2b()
        while chunk := f.read(8192):
            file_hash.update(chunk)
    return file_hash.hexdigest()


def generate_json_history(path):
    files = filter(is_included, glob.iglob(f'{path}/**/*', recursive=True))
    entries_by_date = {}
    for file in files:
        try:
            file_mtime = datetime.fromtimestamp(os.path.getmtime(file))
        except:
            logging.exception("Couldn't get mtime for file {file}")
        else:
            add_entry(
                entries_by_date,
                {
                    'schema': get_schema(file),
                    'title': os.path.basename(file),
                    'description': '',
                    'dateUpdated': to_json_datetime(file_mtime),
                    'path': file,
                    'checksum': generate_checksum(file),
                }
            )

    sorted_entries_by_date = OrderedDict(sorted(entries_by_date.items(), reverse=True))
    sorted_date_keys = list(sorted_entries_by_date.keys())

    output = {
        'dateRange': [sorted_date_keys[-1], sorted_date_keys[0]],
        'dateGenerated': to_json_datetime(datetime.now()),
        'entries': sorted_entries_by_date,
    }
    return output

with open(args.output_path, 'w') as output_file:
    json.dump(generate_json_history(args.source_dir), output_file)